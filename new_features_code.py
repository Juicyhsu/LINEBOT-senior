# ======================
# é€£çµæŸ¥è­‰èˆ‡æ–°èåŠŸèƒ½
# ======================

# ç”¨æˆ¶å¾…è™•ç†é€£çµç‹€æ…‹
user_link_pending = {}
# æ–°èå¿«å– (æ¸›å°‘APIå‘¼å«)
news_cache = {'data': None, 'timestamp': None}
# ç”¨æˆ¶æ–°èå¿«å–(èªéŸ³æ’­å ±)
user_news_cache = {}

def extract_url(text):
    """å¾æ–‡å­—ä¸­æå– URL"""
    import re
    url_pattern = r'https?://[^\s<>"\']+' 
    urls = re.findall(url_pattern, text)
    return urls[0] if urls else None

def extract_domain(url):
    """å¾ URL ä¸­æå–ç¶²åŸŸåç¨±"""
    from urllib.parse import urlparse
    try:
        parsed = urlparse(url)
        return parsed.netloc
    except:
        return None

def check_trusted_media(domain):
    """æª¢æŸ¥æ˜¯å¦ç‚ºå°ç£å¯ä¿¡è³´æ–°èåª’é«”"""
    trusted_domains = [
        'cna.com.tw',  # ä¸­å¤®ç¤¾
        'pts.org.tw',  # å…¬è¦–
        'udn.com',     # è¯åˆæ–°èç¶²
        'ltn.com.tw',  # è‡ªç”±
        'chinatimes.com',  # ä¸­æ™‚
        'ettoday.net', # ETtoday
        'storm.mg',    # é¢¨å‚³åª’
        'setn.com',    # ä¸‰ç«‹æ–°è
        'tvbs.com.tw', # TVBS
        'nownews.com', # ä»Šæ—¥æ–°è
        'rti.org.tw',  # ä¸­å¤®å»£æ’­é›»å°
        'bcc.com.tw',  # ä¸­åœ‹å»£æ’­å…¬å¸
    ]
   
    return any(td in domain.lower() for td in trusted_domains)

def get_domain_age(url):
    """
    æŸ¥è©¢ç¶²åŸŸå¹´é½¡ï¼ˆå¤©æ•¸ï¼‰
    è¿”å›: å¤©æ•¸ (int) æˆ– None (æŸ¥è©¢å¤±æ•—æ™‚)
    """
    try:
        import whois
        from datetime import datetime
        
        domain = extract_domain(url)
        if not domain:
            return None
        
        w = whois.whois(domain)
        
        # whois çš„ creation_date å¯èƒ½æ˜¯ datetime æˆ– list
        creation_date = w.creation_date
        if isinstance(creation_date, list):
            creation_date = creation_date[0]
        
        if creation_date:
            age = (datetime.now() - creation_date).days
            return age
        
        return None
    except Exception as e:
        print(f"Domain age check error: {e}")
        return None

def quick_safety_check(url):
    """
    å¿«é€Ÿå®‰å…¨æª¢æŸ¥
    è¿”å›: {'level': 'safe'|'warning'|'danger', 'risks': [...], 'is_trusted': bool, 'is_scam_like': bool}
    """
    risks = []
    domain = extract_domain(url)
    
    if not domain:
        return {'level': 'warning', 'risks': ['ç„¡æ³•è§£æç¶²åŸŸ'], 'is_trusted': False, 'is_scam_like': False}
    
    # æª¢æŸ¥ 1: å°ç£æ–°èåª’é«”ç™½åå–®
    is_trusted = check_trusted_media(domain)
    
    # æª¢æŸ¥ 2: ç¶²åŸŸå¹´é½¡
    domain_age = get_domain_age(url)
    is_new_domain = False
    if domain_age is not None:
        if domain_age < 90:  # å°‘æ–¼ 3 å€‹æœˆ
            risks.append(f"ç¶²åŸŸè¨»å†Šä¸ä¹… ({domain_age} å¤©)")
            is_new_domain = True
        elif domain_age < 180:  # å°‘æ–¼ 6 å€‹æœˆ
            risks.append(f"ç¶²åŸŸè¼ƒæ–° ({domain_age} å¤©)")
    
    # æª¢æŸ¥ 3: å¯ç–‘é—œéµå­—ï¼ˆè©é¨™å¸¸ç”¨ï¼‰
    scam_keywords = ['éœ‡é©š', 'å¿…çœ‹', 'ä¸å¯æ€è­°', 'é©šäºº', 'å…è²»é€', 'é™æ™‚']
    has_scam_keywords = any(kw in url for kw in scam_keywords)
    if has_scam_keywords:
        risks.append("ç¶²å€åŒ…å«å¯ç–‘é—œéµå­—")
    
    # åˆ¤æ–·æ˜¯å¦æ˜é¡¯åƒè©é¨™
    is_scam_like = is_new_domain and has_scam_keywords
    
    # è©•ä¼°é¢¨éšªç­‰ç´š
    # åªæœ‰ã€Œæ˜é¡¯åƒè©é¨™ã€æ‰è­¦å‘Šï¼Œä¸€èˆ¬ç¶²ç«™ä¸è­¦å‘Š
    if is_scam_like or len(risks) >= 3:
        level = 'danger'
    elif is_new_domain:  # åªæœ‰æ–°ç¶²åŸŸæ‰æé†’
        level = 'warning' 
    else:
        level = 'safe'
    
    return {
        'level': level,
        'risks': risks,
        'is_trusted': is_trusted,
        'is_scam_like': is_scam_like
    }

def format_verification_result(safety_check, url):
    """æ ¼å¼åŒ–æŸ¥è­‰çµæœ"""
    domain = extract_domain(url)
    
    if safety_check['level'] == 'danger':
        return f"""ğŸš¨ ç­‰ç­‰ï¼æˆ‘ç™¼ç¾é€™å€‹é€£çµæœ‰é»å¯ç–‘ï¼š

âš ï¸ é¢¨éšªæç¤ºï¼š
{''.join(['â€¢ ' + risk + '\\n' for risk in safety_check['risks']])}
ğŸ’¡ å»ºè­°å…ˆä¸è¦é»é–‹ï¼

å¦‚æœæ‚¨æƒ³äº†è§£æ›´å¤šï¼Œè«‹å‘Šè¨´æˆ‘æ‚¨çš„éœ€æ±‚ï¼"""
    
    elif safety_check['level'] == 'warning':
        return f"""âš ï¸ æé†’ï¼é€™å€‹ç¶²ç«™æ¯”è¼ƒæ–°ï¼š
{''.join(['â€¢ ' + risk + '\\n' for risk in safety_check['risks']])}
ğŸ’¡ è«‹è¬¹æ…æŸ¥çœ‹ã€‚

æ‚¨æ˜¯æƒ³ï¼š
1ï¸âƒ£ ğŸ” æŸ¥è­‰é€™å€‹é€£çµæ˜¯å¦ç‚ºè©é¨™
2ï¸âƒ£ ğŸ“– è®“æˆ‘å¹«ä½ è®€å…§å®¹

è«‹å‘Šè¨´æˆ‘æ‚¨çš„éœ€æ±‚ï¼"""
    
    else:
        # å°æ–¼ä¸€èˆ¬ç¶²ç«™ï¼Œç›´æ¥è©¢å•æ„åœ–
        return f"""æ”¶åˆ°é€£çµï¼

æ‚¨æ˜¯æƒ³ï¼š
1ï¸âƒ£ ğŸ“– è®“æˆ‘è®€çµ¦ä½ è½ï¼ˆæ‘˜è¦å…§å®¹ï¼‰
2ï¸âƒ£ ğŸ” æŸ¥è­‰é€™å€‹é€£çµ

è«‹å‘Šè¨´æˆ‘ã€Œé–±è®€ã€æˆ–ã€ŒæŸ¥è­‰ã€ï¼"""

def fetch_webpage_content(url):
    """
    æŠ“å–ç¶²é å…§å®¹
    è¿”å›: ç¶²é æ–‡å­—å…§å®¹ (str) æˆ– None
    """
    try:
        from bs4 import BeautifulSoup
        import requests
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        response.encoding = 'utf-8'
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ç§»é™¤ script å’Œ style æ¨™ç±¤
        for script in soup(["script", "style"]):
            script.decompose()
        
        # æå–æ–‡å­—
        text = soup.get_text()
        
        # æ¸…ç†ç©ºç™½
        lines = (line.strip() for line in text.splitlines())
        chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
        text = '\n'.join(chunk for chunk in chunks if chunk)
        
        # é™åˆ¶é•·åº¦ (é¿å…éé•·)
        if len(text) > 5000:
            text = text[:5000] + "..."
        
        return text
    except Exception as e:
        print(f"Fetch webpage error: {e}")
        return None

def summarize_content(content, user_id):
    """ä½¿ç”¨ Gemini æ‘˜è¦ç¶²é å…§å®¹"""
    try:
        prompt = f"""
è«‹å¹«æˆ‘é€™ä½é•·è¼©è®€æ‡‚é€™å€‹ç¶²é ï¼Œç”¨æº«æš–çš„å£å»å‘Šè¨´ä»–ï¼š

{content}

è«‹ç”¨é€™æ¨£çš„æ ¼å¼å›æ‡‰ï¼š

ğŸ“– **å…§å®¹æ‘˜è¦**

ï¼ˆç”¨3-5å¥è©±è§£é‡‹é‡é»ï¼‰

ğŸ’¡ **æˆ‘çš„å»ºè­°**

ï¼ˆå‘Šè¨´é•·è¼©é€™å…§å®¹æ˜¯å¦å¯ä¿¡ï¼Œæœ‰ä»€éº¼è¦æ³¨æ„çš„ï¼‰
"""
        
        response = model.generate_content(prompt)
        return response.text
    except Exception as e:
        print(f"Summarize error: {e}")
        return "æŠ±æ­‰ï¼Œæˆ‘ç„¡æ³•è®€æ‡‚é€™å€‹ç¶²é å…§å®¹ï¼Œè«‹ç¨å¾Œå†è©¦ï¼"

def fetch_latest_news():
    """
    æŠ“å–æœ€æ–°æ–°è(ä½¿ç”¨ RSS)
    è¿”å›: æ–°èåˆ—è¡¨ (list of dict)
    """
    try:
        import feedparser
        from datetime import datetime, timedelta
        
        # æª¢æŸ¥å¿«å– (5 åˆ†é˜å…§ä¸é‡è¤‡æŠ“å–)
        if news_cache['data'] and news_cache['timestamp']:
            if datetime.now() - news_cache['timestamp'] < timedelta(minutes=5):
                return news_cache['data']
        
        feeds = [
            'https://www.cna.com.tw/rss/headline.xml',  # ä¸­å¤®ç¤¾é ­æ¢
            # å¯ä»¥æ·»åŠ æ›´å¤šä¾†æº
        ]
        
        news_items = []
        for feed_url in feeds:
            try:
                feed = feedparser.parse(feed_url)
                for entry in feed.entries[:5]:  # æ¯å€‹ä¾†æºå– 5 å‰‡
                    news_items.append({
                        'title': entry.title,
                        'summary': entry.get('summary', ''),
                        'link': entry.link,
                        'published': entry.get('published', '')
                    })
            except Exception as e:
                print(f"Feed parse error for {feed_url}: {e}")
                continue
        
        # æ›´æ–°å¿«å–
        news_cache['data'] = news_items
        news_cache['timestamp'] = datetime.now()
        
        return news_items
    except Exception as e:
        print(f"Fetch news error: {e}")
        return []

def detect_news_intent(text):
    """æª¢æ¸¬æ˜¯å¦æƒ³è¦æŸ¥è©¢æ–°è"""
    keywords = ['æ–°è', 'æ¶ˆæ¯', 'ç™¼ç”Ÿ', 'ä»Šå¤©', 'æœ€è¿‘', 'æ™‚äº‹', 'é ­æ¢']
    return any(keyword in text for keyword in keywords)

def generate_news_summary():
    """ç”Ÿæˆæ–°èæ‘˜è¦"""
    news_items = fetch_latest_news()
    
    if not news_items:
        return "æŠ±æ­‰ï¼Œç›®å‰ç„¡æ³•å–å¾—æ–°èè³‡è¨Šï¼Œè«‹ç¨å¾Œå†è©¦ï¼"
    
    # ä½¿ç”¨ Gemini ç²¾é¸æ–°è
    try:
        news_text = "\n\n".join([
            f"æ¨™é¡Œ: {item['title']}\nå…§å®¹: {item['summary']}"
            for item in news_items[:6]
        ])
        
        prompt = f"""
è«‹å¾é€™äº›æ–°èä¸­ï¼ŒæŒ‘é¸æœ€é‡è¦çš„ 3 å‰‡
æ¯å‰‡æ‘˜è¦æ§åˆ¶åœ¨50å­—ä»¥å…§ï¼š

{news_text}

æ ¼å¼ï¼š
ğŸ“° ä»Šæ—¥æ–°èæ‘˜è¦

1ï¸âƒ£ ã€æ¨™é¡Œã€‘
   ï¼ˆæ‘˜è¦å…§å®¹...ï¼‰

2ï¸âƒ£ ã€æ¨™é¡Œã€‘
   ï¼ˆæ‘˜è¦å…§å®¹...ï¼‰

3ï¸âƒ£ ã€æ¨™é¡Œã€‘
   ï¼ˆæ‘˜è¦å…§å®¹...ï¼‰
"""
        
        response = model.generate_content(prompt)
        return response.text + "\n\nğŸ”Š è¦èªéŸ³æ’­å ±å—ï¼Ÿèªªã€Œè¦èªéŸ³ã€ï¼"
    except Exception as e:
        print(f"News summary error: {e}")
        return "æŠ±æ­‰ï¼Œç„¡æ³•æ•´ç†æ–°èè³‡è¨Šï¼Œè«‹ç¨å¾Œå†è©¦ï¼"

def generate_news_audio(text, user_id):
    """
    ç”Ÿæˆæ–°èèªéŸ³
    è¿”å›: éŸ³è¨Šæª”è·¯å¾‘ (str) æˆ– None
    """
    try:
        # ä½¿ç”¨ Google Cloud TTS (ä¸­æ–‡å“è³ªå¥½)
        from google.cloud import texttospeech
        
        client = texttospeech.TextToSpeechClient()
        
        synthesis_input = texttospeech.SynthesisInput(text=text)
        voice = texttospeech.VoiceSelectionParams(
            language_code="zh-TW",
            name="cmn-TW-Wavenet-A"  # å°ç£å¥³è²
        )
        audio_config = texttospeech.AudioConfig(
            audio_encoding=texttospeech.AudioEncoding.MP3
        )
        
        response = client.synthesize_speech(
            input=synthesis_input,
            voice=voice,
            audio_config=audio_config
        )
        
        # å„²å­˜éŸ³è¨Š
        os.makedirs(UPLOAD_FOLDER, exist_ok=True)
        audio_path = os.path.join(UPLOAD_FOLDER, f"{user_id}_news.mp3")
        with open(audio_path, 'wb') as f:
            f.write(response.audio_content)
        
        return audio_path
    except Exception as e:
        print(f"TTS error: {e}")
        return None
